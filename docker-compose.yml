services:
  # Ollama service for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: laravel-rag-ollama
    platform: linux/arm64  # M1 Mac compatibility
    volumes:
      - ./models:/root/.ollama  # Mount models to local project folder
    ports:
      - "11434:11434"
    networks:
      - rag-network
    environment:
      - OLLAMA_DEBUG=false
      - OLLAMA_VERBOSE=false
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # Python application for RAG pipeline
  rag-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: laravel-rag-app
    platform: linux/arm64
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./src:/app/src
      - ./config:/app/config
      - ./data:/app/data
      - ./chromadb:/app/chromadb  # Mount ChromaDB to local project folder
      - ./sources:/app/sources    # Mount docs cache to local project folder
      - ./logs:/app/logs
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - CHROMA_PERSIST_DIR=/app/chromadb
      - DOCS_CACHE_DIR=/app/sources
      - LOG_LEVEL=INFO
      - LARAVEL_VERSION=12
      - EMBEDDING_MODEL=nomic-embed-text
      - LLM_MODEL=gemma:2b
      - TOP_K=5
      - RESPONSE_TIMEOUT=30
      - ANONYMIZED_TELEMETRY=False
      - POSTHOG_DISABLED=1
    ports:
      - "8000:8000"  # API interface
    networks:
      - rag-network
    command: ["python", "-m", "uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

networks:
  rag-network:
    driver: bridge
